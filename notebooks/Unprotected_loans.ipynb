{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4932e76c-aaac-40c6-b034-b6c54920cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas numpy seaborn matplotlib scikit-learn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91655c9-5224-4d48-aec3-6956f850d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, ConfusionMatrixDisplay, precision_recall_curve\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389f2e75-5b4a-4fb8-a52e-61bce51f76af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_DS_HW_train.csv',delimiter=';',decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f851f63d-2724-4d28-a6af-590811ce582c",
   "metadata": {},
   "source": [
    "Calculating the percentage of missing data in the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b4ee7-a2af-4d50-9e4d-3e11e46bc2f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def missing_data(df):\n",
    "    missing_percentage = 100 * df.isnull().sum()/len(df)\n",
    "    missing_percentage = missing_percentage[missing_percentage > 0]\n",
    "    return missing_percentage\n",
    "missing_percentage = missing_data(df).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be01bdd-7e96-4b82-a7e7-905c8cd74160",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=missing_percentage.index, y=missing_percentage)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Percentage of Missing Data by Column')\n",
    "plt.ylabel('Percentage of Missing Data')\n",
    "plt.xlabel('Columns')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8657c3-169b-4fb6-95cb-e3a7ff1409b2",
   "metadata": {},
   "source": [
    "This cell was used for desplaying columns with different percentage of NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855c30c0-3a18-41ce-8d97-7867d118250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing_percentage[missing_percentage<5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f90c90c-895e-4e4f-b431-2a1052ebac63",
   "metadata": {},
   "source": [
    "Dealing with missing Data - I dealt with NAs using different methods depending on type of data(categorical/numerical), percentage of NAs,\n",
    "but also my assumptions what the columns represent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1a1bf6-c795-45b8-9a2b-5139ace47ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the rows where NA < 1%\n",
    "col_low_na = missing_percentage[missing_percentage < 1].index\n",
    "df = df.dropna(axis=0, subset=col_low_na)\n",
    "\n",
    "# Replacing NA with 0 in columns in which there is assumption based on the business intuition NA stands for 0\n",
    "num_columns_to_fill = ['min_prepaid_days', 'max_prepaid_days', 'avg_prepaid_days','swo_ind_loans_number','swo_ind_closed_paid_loans_lenders_number','swo_ind_active_loans_amount','policyrule_partialdecision','swo_ind_overdue_loans_number','swo_ind_same_ip_different_pesel_count','swo_ind_same_ip_different_pesel_count_unique']\n",
    "df[num_columns_to_fill] = df[num_columns_to_fill].fillna(0)\n",
    "\n",
    "# Replacing NA with None(Lack of information might be meaningful) \n",
    "text_columns_to_fill = ['citysize','companiesinhouse','householdsize','housesize','legalform','policyrule_partialdecision']\n",
    "df[text_columns_to_fill] = df[text_columns_to_fill].fillna('None')\n",
    "\n",
    "# Marking missing numeric values and filling it with median\n",
    "mark_miss_num_values = ['num_paid_loans_with_dpd_to_all','num_paid_loans_on_time','num_paid_loans_with_dpd','num_prepaid_loans','daysafterregistration','max_dpd','avg_dpd','days_after_knownsince_pbd']\n",
    "for col in mark_miss_num_values:\n",
    "    df[f'{col}_missing'] = df[col].isnull().astype(int)\n",
    "for col in mark_miss_num_values:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# Marking missing numeric values and filling it with 0\n",
    "mark_num_values = ['amounttotal','amounttotalopen','count','creditstatusmax','num_positive_accounts','sum_positive_accounts','chap_countrejectedapplications']\n",
    "for col in mark_num_values:\n",
    "    df[f'{col}_missing'] = df[col].isnull().astype(int)\n",
    "for col in mark_num_values:\n",
    "    df[col] = df[col].fillna(0)\n",
    "\n",
    "# The k-nearest neighbors\n",
    "knn_miss_data = ['swo_ind_active_loans_amount','chap_coef','swo_ind_active_loans_amount','swo_ind_active_loans_open_amount','num_positive_accounts','sum_positive_accounts','days_after_first_positive_accounts','days_after_last_positive_accounts','swo_ind_active_loans_number']\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df[knn_miss_data] = imputer.fit_transform(df[knn_miss_data])\n",
    "\n",
    "# Removing date\n",
    "if 'loan_date_created' in df.columns:\n",
    "    df = df.drop(['loan_date_created'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6bd9c4-1404-49ed-8ad1-98f2b539798a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Separating features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fc3298-b0f2-482f-b073-6032fdabc182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df.drop(['TARGET', 'application_id'], axis=1)\n",
    "y = df['TARGET']\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "numerical_cols = X.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "# Hot encoding categorical columns\n",
    "X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2ba920-df38-44f1-a3d8-39a2c76ea278",
   "metadata": {},
   "source": [
    "Dealing with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83dccb8-f50f-4e5c-a9d0-8f9a5b2c7cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Z-scores\n",
    "z_scores = np.abs(stats.zscore(df.select_dtypes(include=[np.number])))\n",
    "\n",
    "# Identifing outliers\n",
    "outliers = (z_scores > 3)\n",
    "outliers_count = np.sum(outliers, axis=0)\n",
    "outliers_per = outliers_count/len(df)\n",
    "\n",
    "print(\"Percentage of outliers in each numeric column:\")\n",
    "print(outliers_per.sort_values().tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e79a9-3cfa-44a4-ab93-76d1dde2364f",
   "metadata": {},
   "source": [
    "I would not remove outliers looking at the character of the data and regarding the fact I am planning to use random\n",
    "forest which is not sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341bbeb7-9079-4e2a-b785-1dd2742b2891",
   "metadata": {},
   "source": [
    "Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095e5b22-bf34-45ca-8c0e-62481bd066b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30806f6-b5d3-4b5f-9799-53d3aaa06330",
   "metadata": {},
   "source": [
    "I tuned a RandomForest using grid_search algorithm. It took a minute so I am giving the result of a grid search below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9640a10-a6cc-42d7-a0d8-30bef9ebb01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #finding best parameters for random forest\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200],\n",
    "#     'max_depth': [20, 30, None],\n",
    "#     'min_samples_split': [2, 3, 4],\n",
    "#     'min_samples_leaf': [1, 2]\n",
    "# }\n",
    "# grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "#                            param_grid=param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=2)\n",
    "\n",
    "# grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6870c71-5abf-4c28-b74d-70f6b7e9ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 200}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ebe9a-ebd2-4acf-b173-3ab21214ffab",
   "metadata": {},
   "source": [
    "Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded799ed-fe39-4056-870e-17ee37c1cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = RandomForestClassifier(class_weight='balanced', max_depth=30, min_samples_split=3, n_estimators=200, random_state=42)\n",
    "best_rf.fit(X_train, y_train)\n",
    "y_pred = best_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa74448-ca4b-465d-8671-2a2fdbe00067",
   "metadata": {},
   "source": [
    "ROC Curve and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8b218e-1926-407b-b5eb-c4f47dcabc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision, recall, and thresholds\n",
    "y_pred_proba = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "precisions, recalls, thresholds_pr = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "# F1-scores for each threshold\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
    "\n",
    "# Finding the threshold with the maximum F1-score\n",
    "best_f1_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds_pr[best_f1_idx]\n",
    "\n",
    "print(f\"Best threshold based on F1-score: {best_threshold:.2f}\")\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76028ac-41e5-443f-abb1-788fefb7d26a",
   "metadata": {},
   "source": [
    "AUC = 0.92 is a very good result. It means the model has a high ability to distinguish between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de84ea7-99f4-416b-915b-c68f0143cbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation \n",
    "y_pred_proba = best_rf.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_pred_proba >= best_threshold).astype(int)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31284c4d-f38a-4c7b-b043-e87c9d2d1e23",
   "metadata": {},
   "source": [
    "The threshold should be set based on the potential gain from the interest rate of the loan compared to the potential loss from defaults. This ensures that the threshold aligns with the financial trade-offs between expected revenue and the risk of loan defaults. However, due to the high threshold that maximizes the F1 score, the model rarely flags the risk of default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9973c4c1-bc0e-4e0e-9275-fb3b7186e7d5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Predicting values in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765241ab-5a21-4c04-a1a4-b32dd4f2b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparing test set\n",
    "df_test = pd.read_csv('data_DS_HW_test.csv', delimiter=';', decimal=',')\n",
    "\n",
    "# Applying the same transformations to the test data\n",
    "# Handling missing data (same as done for training data)\n",
    "df_test[num_columns_to_fill] = df_test[num_columns_to_fill].fillna(0)\n",
    "df_test[text_columns_to_fill] = df_test[text_columns_to_fill].fillna('None')\n",
    "\n",
    "for col in mark_miss_num_values:\n",
    "    df_test[f'{col}_missing'] = df_test[col].isnull().astype(int)\n",
    "df_test[mark_miss_num_values] = df_test[mark_miss_num_values].fillna(df_test[mark_miss_num_values].median())\n",
    "\n",
    "for col in mark_num_values:\n",
    "    df_test[f'{col}_missing'] = df_test[col].isnull().astype(int)\n",
    "df_test[mark_num_values] = df_test[mark_num_values].fillna(0)\n",
    "\n",
    "df_test[knn_miss_data] = imputer.transform(df_test[knn_miss_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de9570d-d912-4ca0-93b5-f1853dfab2e4",
   "metadata": {},
   "source": [
    "Separating features (no 'Target' column here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df11e48-01b6-4291-bb31-07315047e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.drop(['application_id','loan_date_created'], axis=1)\n",
    "\n",
    "# One-hot encoding categorical columns\n",
    "X_test = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Aligning columns with training data\n",
    "missing_cols = set(X.columns) - set(X_test.columns)\n",
    "for col in missing_cols:\n",
    "    X_test[col] = 0\n",
    "X_test = X_test[X.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b7b8f9-9773-43e5-a4e3-8ed7d427df93",
   "metadata": {},
   "source": [
    "New model trained on the whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd86c21a-7974-4646-8241-f2b26b374316",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_new = RandomForestClassifier(class_weight='balanced', max_depth=30,\n",
    "                       min_samples_split=3, n_estimators=200, random_state=42)\n",
    "rf_new.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072c3571-f39a-4a1e-9f3a-5649996da4b3",
   "metadata": {},
   "source": [
    "Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37d84e6-92b4-4e50-bdc9-1fa7beee95e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_proba = rf_new.predict_proba(X_test)[:, 1]\n",
    "df_test['probability_of_default'] = y_test_pred_proba.round(3)\n",
    "original_test_data = pd.read_csv('data_DS_HW_test.csv', delimiter=';', decimal=',')\n",
    "# loading results to csv\n",
    "pd.concat([original_test_data, df_test['probability_of_default']], axis=1).to_csv('data_DS_HW_test_with_probabilities.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea2e2ea-3c58-406d-9c53-3ac35d577f17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
